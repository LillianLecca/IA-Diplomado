{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "NaiveBayes.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc6DAXsLSpHZ",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "El objetivo del notebook es experimentar con el clasificador Naive Bayes en sus diferentes implementaciones. Se utilizará el clasificador gausiano para predicción multiclase en el conjunto iris y el clasficiador multinomial para clasficiación de mensajes de texto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-JgWVm9P_VD",
        "colab_type": "text"
      },
      "source": [
        "## Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgNXQeazL4Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTr5Pg8QL5z",
        "colab_type": "text"
      },
      "source": [
        "Función de utilizad para visualizar matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng_Cg0kZQLK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
        "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    confusion_matrix: numpy.ndarray\n",
        "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
        "        Similarly constructed ndarrays can also be used.\n",
        "    class_names: list\n",
        "        An ordered list of class names, in the order they index the given confusion matrix.\n",
        "    figsize: tuple\n",
        "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
        "        the second determining the vertical size. Defaults to (10,7).\n",
        "    fontsize: int\n",
        "        Font size for axes labels. Defaults to 14.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.figure.Figure\n",
        "        The resulting confusion matrix figure\n",
        "        \n",
        "    Reference\n",
        "    -------\n",
        "    https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
        "    \n",
        "    \"\"\"\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names, \n",
        "    )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    print(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iLiClhnPsy2",
        "colab_type": "text"
      },
      "source": [
        "### Carga del dataset y exploración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qiol9N3N0GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris['feature_names'] + ['target'])\n",
        "iris_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGL7zs4Mvwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHo0xjfvNGIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_df.hist()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA-KVPKxPunR",
        "colab_type": "text"
      },
      "source": [
        "### Separación en cojuntos de entrenamiento y validación, normalización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKhFJ_qwNcuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris_df[iris['feature_names']].values\n",
        "y = iris_df['target'].values\n",
        " \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MGkNC7fPzBf",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento y evaluación de un clasificador gausiano"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2YiJsghOLFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gnb = GaussianNB()\n",
        "model = gnb.fit(X_train, y_train)\n",
        "print('Accuracy of GaussianNB classifier on training set: {:.2f}'\n",
        "     .format(gnb.score(X_train, y_train)))\n",
        "print('Accuracy of GaussianNB classifier on test set: {:.2f}'\n",
        "     .format(gnb.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48yU3iMxOn3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = gnb.predict(X_test)\n",
        "print_confusion_matrix(confusion_matrix(y_test, pred),[\"Setosa\",\"Versicolor\",\"Virginica\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCVe0NbIqaN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_t = gnb.predict(X_train)\n",
        "print_confusion_matrix(confusion_matrix(y_train, pred_t),[\"Setosa\",\"Versicolor\",\"Virginica\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfhj1kRiPTUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqG6b79qwNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_train, pred_t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5IUDLodFRtp",
        "colab_type": "text"
      },
      "source": [
        "## Automated SMS spam filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJfUEqqnMIwP",
        "colab_type": "text"
      },
      "source": [
        "Adaptado de https://radimrehurek.com/data_science_python/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpHGnp4KFRtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA9lSKZoFRtv",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Load data, look around"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnuOkdWaFRtw",
        "colab_type": "text"
      },
      "source": [
        "Skipping the *real* first step (fleshing out specs, finding out what is it we want to be doing -- often highly non-trivial in practice!), let's download the dataset we'll be using in this demo. Go to https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection and download the zip file. Unzip it under `data` subdirectory. You should see a file called `SMSSpamCollection`, about 0.5MB in size:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG5mqyH0FRtx",
        "colab_type": "text"
      },
      "source": [
        "```bash\n",
        "$ ls -l data\n",
        "total 1352\n",
        "-rw-r--r--@ 1 kofola  staff  477907 Mar 15  2011 SMSSpamCollection\n",
        "-rw-r--r--@ 1 kofola  staff    5868 Apr 18  2011 readme\n",
        "-rw-r-----@ 1 kofola  staff  203415 Dec  1 15:30 smsspamcollection.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjHrS1obAejI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzjt_Yk4Anet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip smsspamcollection.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktpu-C8iFRtx",
        "colab_type": "text"
      },
      "source": [
        "This file contains **a collection of more than 5 thousand SMS phone messages** (see the `readme` file for more info):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z698-hsQFRty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages = [line.rstrip() for line in open('SMSSpamCollection')]\n",
        "len(messages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxsqxXJQFRt0",
        "colab_type": "text"
      },
      "source": [
        "A collection of texts is also sometimes called \"corpus\". Let's print the first ten messages in this SMS corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdCKE4QsFRt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for message_no, message in enumerate(messages[:10]):\n",
        "    print (message_no, message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH15u_XsFRt3",
        "colab_type": "text"
      },
      "source": [
        "We see that this is a [TSV](http://en.wikipedia.org/wiki/Tab-separated_values) (\"tab separated values\") file, where the first column is a label saying whether the given message is a normal message \"ham\" or \"spam\". The second column is the message itself.\n",
        "\n",
        "This corpus will be our labeled training set. Using these ham/spam examples, we'll **train a machine learning model to learn to discriminate between ham/spam automatically**. Then, with a trained model, we'll be able to **classify arbitrary unlabeled messages** as ham or spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCt9UHZ7FRt4",
        "colab_type": "text"
      },
      "source": [
        "[![](http://radimrehurek.com/data_science_python/plot_ML_flow_chart_11.png)](http://www.astroml.org/sklearn_tutorial/general_concepts.html#supervised-learning-model-fit-x-y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KfUNG6xFRt4",
        "colab_type": "text"
      },
      "source": [
        "Instead of parsing TSV (or CSV, or Excel...) files by hand, we can use Python's `pandas` library to do the work for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHEYcjWcFRt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages = pd.read_csv('SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
        "                           names=[\"label\", \"message\"])\n",
        "messages.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-zArxgFRt9",
        "colab_type": "text"
      },
      "source": [
        "With `pandas`, we can also view aggregate statistics easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqqVdJ6MFRt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.groupby('label').describe().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAm8EcpQFRuA",
        "colab_type": "text"
      },
      "source": [
        "How long are the messages?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hNmRprFRuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages['length'] = messages['message'].map(lambda text: len(text))\n",
        "messages.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLECdpO5FRuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.length.plot(bins=20, kind='hist');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BoTT3byFRuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.length.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTVeCgOFRuI",
        "colab_type": "text"
      },
      "source": [
        "What is that super long message?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpSPZLgHFRuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (list(messages.message[messages.length > 900]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF0gG_6xFRuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (list(messages[messages.length > 900].index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiYjH8ytFRuN",
        "colab_type": "text"
      },
      "source": [
        "Is there any difference in message length between spam and ham?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fALEpwBFFRuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.hist(column='length', by='label', bins=50, figsize=(12,4));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHtWGmZxFRuP",
        "colab_type": "text"
      },
      "source": [
        "Good fun, but how do we make computer understand the plain text messages themselves? Or can it under such malformed gibberish at all?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3EMus0RFRuQ",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Data to vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX56EDmSFRuR",
        "colab_type": "text"
      },
      "source": [
        "Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
        "\n",
        "Doing that requires essentially three steps, in the bag-of-words model:\n",
        "\n",
        "1. counting how many times does a word occur in each message (term frequency)\n",
        "2. weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
        "3. normalizing the vectors to unit length, to abstract from the original text length (L2 norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX989NTGFRuR",
        "colab_type": "text"
      },
      "source": [
        "Each vector has as many dimensions as there are unique words in the SMS corpus.\n",
        "\n",
        "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahGJDvP8FRuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "sms_tfidf = vectorizer.fit_transform(messages['message'].values)\n",
        "\n",
        "print(sms_tfidf.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Vw7R9mFRuV",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Training a model, detecting spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXi5uINAFRuV",
        "colab_type": "text"
      },
      "source": [
        "With messages represented as vectors, we can finally train our spam/ham classifier. This part is pretty straightforward, and there are many libraries that realize the training algorithms.\n",
        "The library sklearn.naive_bayes includes implementations of:\n",
        "- GaussianNB\n",
        "- MultinomialNB \n",
        "- BernoulliNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjAIgmuCF83c",
        "colab_type": "text"
      },
      "source": [
        "#### What classifier class should we use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIWcpnfMFRuW",
        "colab_type": "text"
      },
      "source": [
        "#### When are used the other two NB versions? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nj2KZJUFRuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "targets = messages['label'].values\n",
        "clf = classifier.fit(sms_tfidf, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcO_mIorFRuZ",
        "colab_type": "text"
      },
      "source": [
        "Let's try classifying our single random message:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbMFnI_sFRua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = ['Free entry in 3 a wkly comp', 'Hello my friend']\n",
        "example_vector = vectorizer.transform(examples)\n",
        "predictions = classifier.predict(example_vector)\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AavbuHV2FRug",
        "colab_type": "text"
      },
      "source": [
        "Hooray! You can try it with your own texts, too.\n",
        "\n",
        "A natural question is to ask, how many messages do we classify correctly overall?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsLF_g9-FRuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "all_predictions = clf.predict(sms_tfidf)\n",
        "accuracy = accuracy_score(messages['label'], all_predictions)\n",
        "cm = confusion_matrix(messages['label'], all_predictions)\n",
        "statistics = classification_report(messages['label'], all_predictions)\n",
        "\n",
        "print('Accuracy: %.4f\\n' % accuracy)\n",
        "print(statistics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZScGvARHWkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_confusion_matrix(cm,[\"ham\", \"spam\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVHbIvmlFRul",
        "colab_type": "text"
      },
      "source": [
        "#### By default, MultinomialNB uses the Additive Laplace smoothing (alpha = 1). Change the classifier to work with Lidstone smoothing. Explain new results comparing with the default version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLyt59QFRun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = MultinomialNB(alpha=0.01)\n",
        "targets = messages['label'].values\n",
        "clf = classifier.fit(sms_tfidf, targets)\n",
        "\n",
        "all_predictions = clf.predict(sms_tfidf)\n",
        "accuracy = accuracy_score(messages['label'], all_predictions)\n",
        "cm = confusion_matrix(messages['label'], all_predictions)\n",
        "statistics = classification_report(messages['label'], all_predictions)\n",
        "\n",
        "print('Accuracy: %.4f\\n' % accuracy)\n",
        "print(statistics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URbncewLI9Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_confusion_matrix(cm,[\"ham\", \"spam\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}