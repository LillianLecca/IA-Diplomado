{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "PCA.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpZlRwH64nbv",
        "colab_type": "text"
      },
      "source": [
        "# Ejercicio en clase: Análisis de Componentes Principales\n",
        "\n",
        "## 1. PCA con `scikit-learn`: Conjunto de datos USArrests\n",
        "\n",
        "Versión original en R: James, G., Witten, D., Hastie, T., & Tibishirani, R. (2013). An Introduction to Statistical Learning. Springer Texts in Statistics.\n",
        "\n",
        "Adaptación original a Python por J. Warmenhoven, actualizada por R. Jordan Crouser.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zKvGJEs4nbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdRAWrOl3nV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1xKSc7x4ZjcmL-x_w2yGRUAVNdHcWgLcD' -O USArrests.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvOkS6B14nby",
        "colab_type": "text"
      },
      "source": [
        "En este ejercicio realizamos Análisis de Componentes Principales (PCA) en el conjunto de datos `USArrests`. Para cada uno de los 50 estados de los EE.UU., el conjunto de datos contiene el número de arrestos por cada 100,000 residentes para cada uno de tres crímenes: Asalto (`Assault`), Asesinato (`Murder`) y Violación (`Rape`). También se registra la variable `UrbanPop` (porcentaje de la población en cada estado que vive en las áreas urbanas).\n",
        "\n",
        "Las filas del conjunto de datos contienen los 50 estados, en orden alfabético."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFCX2BPa4nbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('USArrests.csv', index_col=0)\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEiQE4Jx4nb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtymjEH34nb4",
        "colab_type": "text"
      },
      "source": [
        "Empecemos por dar una rápida mirada al valor medio de cada columna:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDwQw7WL4-iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmSDzAUb4nb7",
        "colab_type": "text"
      },
      "source": [
        "Vemos de inmediato que los datos tienen valores medios **notablemente** diferentes. También podemos examinar la varianza de las cuatro variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvH8gYP-4nb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.var()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4KOp5W4nb-",
        "colab_type": "text"
      },
      "source": [
        "Como podría haberse esperado, las variables tienen también varianzas notablemente diferentes: la variable `UrbanPop` mide el porcentaje de la población en cada estado que vive en un área urbana, número que no es comparable con el número de crímenes cometidos en cada estado por cada 100,000 individuos. Si dejásemos de escalar las variables antes de realizar PCA, la mayoría de los componentes principales que observaríamos estarían sesgados hacia la variable `Assault`, pues ésta tiene ampliamente la mayor media y la mayor varianza.\n",
        "\n",
        "Así pues, es importante estandarizar las variables con una media de cero y una desviación estándar de 1 antes de realizar PCA. Podemos hacerlo usando la función `scale` de `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wYkE0VR4nb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "X = scale(df)\n",
        "\n",
        "# La función scale es equivalente a:\n",
        "# X = (df - np.mean(df, axis=0)) / np.std(df, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz7SPXW14ncB",
        "colab_type": "text"
      },
      "source": [
        "Visualicemos los primeros 5 estados de `X` en un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpZZG8N4ncC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(X, index=df.index, columns=df.columns).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5nsRZlu4ncE",
        "colab_type": "text"
      },
      "source": [
        "Ahora usaremos la función `PCA()` de `sklearn` para calcular los vectores de cargas *(loadings)*, es decir, los coeficientes que cada variable aporta a cada uno de los componentes principales. Una vez ajustado el modelo, los vectores de carga pueden recuperarse con el atributo `components_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vllA3Jht4ncE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA().fit(X)\n",
        "pca_loadings = pca.components_\n",
        "\n",
        "# También podríamos haberlo calculado usando Singular Value Decomposition\n",
        "# (U, s, Vt) = np.linalg.svd(X)\n",
        "# pca_loadings = Vt\n",
        "\n",
        "# Mostramos las cargas de cada componente principal en un DataFrame\n",
        "pd.DataFrame(pca_loadings, columns=df.columns, index=['PC1', 'PC2', 'PC3', 'PC4']).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKPE7Dzl4ncH",
        "colab_type": "text"
      },
      "source": [
        "Vemos que hay cuatro componentes principales. Esto es de esperarse, porque hay en general $min(n-1, p)$ componentes principales informativos en un conjunto de datos de $n$ observaciones y $p$ variables.\n",
        "\n",
        "La matriz de vectores de cargas es a veces denominada *matriz de rotación*, porque cuando multiplicamos la matriz X por la transpuesta de la matriz de cargas, nos da las coordenadas de los datos en el sistema de coordenadas rotado. Estas coordenadas son los puntajes *(scores)* de los componentes principales.\n",
        "\n",
        "Usando la función `transform` de `sklearn`, no necesitamos multiplicar explícitamente los datos por los vectores de carga de los componentes principales para obtener los vectores de puntaje de los componentes principales. La función `transform` nos devuelve una matriz de dimensiones 50 x 4 cuyas columnas son los vectores de puntaje de los componentes principales. Es decir, la k-ésima columna corresponde al vector de puntajes del k-mo componente principal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyiL-07j4ncH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Usando sklearn\n",
        "pca_scores = pca.transform(X)\n",
        "\n",
        "# Usando Singular Value Decomposition, los puntajes se obtienen multiplicando U * Sigma\n",
        "# (Primero reconstruimos la matriz Sigma a partir de los valores singulares s)\n",
        "# Sigma = np.zeros(X.shape, dtype='float64')   \n",
        "# Sigma[:X.shape[1],:X.shape[1]] = np.diag(s)\n",
        "# pca_scores = np.dot(U, Sigma)\n",
        "\n",
        "# O también multiplicando X * V\n",
        "# V = Vt.T\n",
        "# pca_scores = np.dot(X, V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHtoZ4HS5hE7",
        "colab_type": "text"
      },
      "source": [
        "Visualizamos los puntajes para los primeros estados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y75-k-Rk4ncJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot = pd.DataFrame(pca_scores, columns=['PC1', 'PC2', 'PC3', 'PC4'], index=df.index)\n",
        "df_plot.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AbY7A_Y4ncL",
        "colab_type": "text"
      },
      "source": [
        "Como podemos verificar, es posible recuperar por completo el conjunto de datos inicial multiplicando la matriz de puntajes $(U \\Sigma)$ por la matriz de cargas $(V^\\top)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGTbCntC4ncM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_rec = np.dot(pca_scores, pca_loadings)\n",
        "pd.DataFrame(X_rec, index=df.index, columns=df.columns).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i__G8Y1i4ncO",
        "colab_type": "text"
      },
      "source": [
        "Definamos una función para construir un **biplot** de los dos primeros componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vgI7Jn4ncP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def biplot(loadings, scores, index, columns):\n",
        "    fig , ax1 = plt.subplots(figsize=(9,7))\n",
        "\n",
        "    ax1.set_xlim(-3.5,3.5)\n",
        "    ax1.set_ylim(-3.5,3.5)\n",
        "\n",
        "    # Nombre de cada estado ubicado por puntajes para cada componente principal\n",
        "    for i, index in enumerate(index):\n",
        "        ax1.annotate(index, (scores[i, 0], scores[i, 1]), ha='center', color='blue')\n",
        "\n",
        "    # Líneas de referencia\n",
        "    ax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
        "    ax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
        "\n",
        "    ax1.set_xlabel('Primer Componente Principal')\n",
        "    ax1.set_ylabel('Segundo Componente Principal')\n",
        "\n",
        "    # Diagramar los vectores de cargas, superponiendo un segundo eje x, y\n",
        "    ax2 = ax1.twinx().twiny() \n",
        "\n",
        "    ax2.set_ylim(-1,1)\n",
        "    ax2.set_xlim(-1,1)\n",
        "    ax2.set_xlabel('Vectores de cargas de los componentes principales', color='red')\n",
        "\n",
        "    # Vectores de carga\n",
        "    # La variable 'a' es un pequeño offset para separar las etiquetas de las flechas\n",
        "    a = 1.07  \n",
        "    for i, column in enumerate(columns):\n",
        "        ax2.annotate(column, (loadings[0,i]*a, loadings[1,i]*a), color='red')\n",
        "        ax2.arrow(0, 0, loadings[0,i], loadings[1,i], color='red')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-acyJKY4ncX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biplot(loadings=pca_loadings, scores=pca_scores, index=df.index, columns=df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLFb1FmN4ncd",
        "colab_type": "text"
      },
      "source": [
        "La función `PCA()` también nos brinda la varianza explicada por cada componente principal. Podemos acceder a estos valores como sigue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6urshbWP4ncd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "explained_variance = pca.explained_variance_\n",
        "\n",
        "# O, usando SVD:\n",
        "# explained_variance = (s * s) / X.shape[0]\n",
        "\n",
        "print('Varianza explicada por cada componente principal:')\n",
        "print(explained_variance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfttzWh4ncg",
        "colab_type": "text"
      },
      "source": [
        "También podemos obtener la proporción de varianza explicada por cada componente principal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cJlBW3I4nch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PVE = pca.explained_variance_ratio_\n",
        "\n",
        "# O, lo que es lo mismo:\n",
        "# PVE = explained_variance / np.sum(explained_variance)\n",
        "\n",
        "print('Proporción de varianza explicada (PVE) por cada componente principal:')\n",
        "print(PVE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFvp-ay04ncj",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que el primer componente principal explica 62.0% de la varianza en los datos, el siguiente componente principal explica el 24.7% de la varianza, y así en adelante. Podemos diagramar la proporción de varianza explicada (PVE) por cada componente, como sigue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5S_5oU94ncl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot([1,2,3,4], pca.explained_variance_ratio_, '-o')\n",
        "plt.ylabel('Proporción de Varianza Explicada')\n",
        "plt.xlabel('Componente Principal')\n",
        "plt.xlim(0.75,4.25)\n",
        "plt.ylim(0,1.05)\n",
        "plt.xticks([1,2,3,4])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8NvqW494ncq",
        "colab_type": "text"
      },
      "source": [
        "También podemos usar la función `numpy.cumsum()`, que calcula la suma acumulada de los elementos de un vector numérico, para diagramar la PVE acumulada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDKqqsP4ncr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot([1,2,3,4], np.cumsum(pca.explained_variance_ratio_), '-s')\n",
        "plt.ylabel('Proporción Acumulada de Varianza Explicada')\n",
        "plt.xlabel('Componente Principal')\n",
        "plt.xlim(0.75,4.25)\n",
        "plt.ylim(0,1.05)\n",
        "plt.xticks([1,2,3,4])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9dCvOo4ncu",
        "colab_type": "text"
      },
      "source": [
        "# 2. Reducción de dimensionalidad con PCA\n",
        "\n",
        "(Traducido y adaptado del curso \"Machine Learning\" de Andrew NG)\n",
        "\n",
        "En este ejercicio, usarás PCA para realizar reducción de dimensionalidad. Primero experimentarás con un conjunto de datos sencillo en 2D para afianzar la intuición de cómo funciona esta reducción de dimensionalidad, y luego usarás PCA en un conjunto de datos más grande que contiene 5000 rostros.\n",
        "\n",
        "## 2.1. Conjunto de datos 2D\n",
        "\n",
        "Empezaremos con un conjunto de datos en 2D que tiene mucha varianza en una dirección y menor variación en la otra. A continuación cargaremos y visualizaremos el conjunto de datos. En esta parte del ejercicio visualizarás lo que ocurre cuando reduces los datos de 2D a 1D. En la práctica, querrás reducir datos de 256 a 50 dimensiones, por decir algo; pero usar datos en menores dimensiones, como en este ejemplo, nos permite visualizar mejor los algoritmos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98vkwrbb6qkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1Up7hNMhMlzBdDxUJR7l0YbRSM6vB4EYA' -O data1.mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jd4JnD44ncv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.io import loadmat\n",
        "ex7data1 = loadmat('data1.mat')\n",
        "X = np.array(ex7data1['X'])\n",
        "plt.axes().set_aspect('equal', 'datalim')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.scatter(X[:,0], X[:,1], marker='o', cmap='prism')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKGp-c44ncx",
        "colab_type": "text"
      },
      "source": [
        "Como hemos visto, es importante normalizar primero los datos sustrayendo el valor medio de cada característica del conjunto de datos y escalando cada dimensión para que todas estén en el mismo rango. Es lo que hacemos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWIim27Z4ncy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale \n",
        "\n",
        "X_norm = scale(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2DRYENS4nc0",
        "colab_type": "text"
      },
      "source": [
        "Veamos cómo los datos se encuentran ahora normalizados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTavuFVm4nc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Antes de normalizar:\\n', X[:5])\n",
        "print('\\nDespués de normalizar:\\n', X_norm[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-EZ_LeN4nc3",
        "colab_type": "text"
      },
      "source": [
        "Una vez que los datos están normalizados, puedes ejecutar PCA para calcular los componentes principales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxmJpDDP4nc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca2 = PCA().fit(X_norm)\n",
        "pca2_loadings = pca2.components_\n",
        "\n",
        "print('El vector de cargas del primer componente principal es:')\n",
        "print(pca2_loadings[0,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rji1rOdD4nc6",
        "colab_type": "text"
      },
      "source": [
        "Grafiquemos la dirección de los dos primeros componentes principales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6VIIBvl4nc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.axes().set_aspect('equal', 'datalim')\n",
        "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', cmap='prism')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "a = 1.2\n",
        "plt.annotate('PCA1', (pca2_loadings[0,0]*pca2.explained_variance_[0]*a, pca2_loadings[0,1]*pca2.explained_variance_[0]*a), color='red', ha='right')\n",
        "plt.arrow(0,0, pca2_loadings[0,0]*pca2.explained_variance_[0], pca2_loadings[0,1]*pca2.explained_variance_[0], color='r', head_width=0.1, length_includes_head=True)\n",
        "plt.annotate('PCA2', (pca2_loadings[1,0]*pca2.explained_variance_[1]*a, pca2_loadings[1,1]*pca2.explained_variance_[1]*a), color='red', ha='right')\n",
        "plt.arrow(0,0, pca2_loadings[1,0]*pca2.explained_variance_[1], pca2_loadings[1,1]*pca2.explained_variance_[1], color='r', head_width=0.1, length_includes_head=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp6uFXIw4nc8",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Proyección de los datos sobre los componentes principales\n",
        "\n",
        "Después de calcular los componentes principales, puedes usarlos para reducir la dimensión de características de tu conjunto de datos proyectando cada ejemplo sobre un espacio de menores dimensiones, $x^{(i)} \\rightarrow z^{(i)}$ (p.ej., proyectando los datos de 2D a 1D).\n",
        "\n",
        "En la práctica, si estuvieses usando un algoritmo de aprendizaje tal como regresión lineal o quizás redes neuronales, podrías usar los datos proyectados en lugar de los datos originales. Usando los datos proyectados, puedes entrenar tu modelo más rápido pues hay menos dimensiones en los datos de entrada.\n",
        "\n",
        "Para proyectar los datos sólo necesitas truncar las primeras `K` columnas de la matriz de puntajes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s6PAiwZ4nc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def projectData(pcaModel, X, K):\n",
        "    Z = pcaModel.transform(X)[:,:K]\n",
        "    return Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9KUvkTq4ndB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Proyectamos los datos\n",
        "Z = projectData(pca2, X_norm, 1)\n",
        "\n",
        "print('La proyección del primer ejemplo sobre la primera dimensión es: %0.5f' % Z[0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbAaFfr_4ndD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Reconstrucción de una aproximación de los datos\n",
        "\n",
        "Luego de proyectar los datos en un espacio de menores dimensiones, puedes recuperar aproximadamente los datos proyectándolos de regreso en el espacio original de alta dimensión. Para hacerlo, multiplicamos Z por las primeras `K` filas de la matriz de carga."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P3sHTty4ndE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recoverData(pcaModel, Z):\n",
        "    K = Z.shape[1]\n",
        "    X_rec = np.dot(Z, pcaModel.components_[:K,:])\n",
        "    return X_rec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "citl8jiX4ndH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_rec = recoverData(pca2, Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDp_9SHK4ndJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.4. Visualización de las proyecciones\n",
        "\n",
        "Una vez realizada la proyección y reconstrucción aproximada de los datos, veamos en el siguiente diagrama cómo la proyección afecta los datos. Los puntos de datos originales están indicados con color azul, mientras que los puntos de datos proyectados están indicados con color rojo. La proyección retiene efectivamente la información en la dirección del primer componente principal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k11p0wUS4ndK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.axes().set_aspect('equal', 'datalim')\n",
        "for i in range(X_norm.shape[0]):\n",
        "    plt.arrow(X_norm[i,0], X_norm[i,1], X_rec[i,0]-X_norm[i,0], X_rec[i,1]-X_norm[i,1], color='k')\n",
        "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', c='blue', label='Datos originales')  \n",
        "plt.scatter(X_rec[:,0], X_rec[:,1], marker='o', c='red', label='Datos recuperados')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxA7ky8H4ndM",
        "colab_type": "text"
      },
      "source": [
        "## 2.5. Conjunto de datos de imágenes de rostros\n",
        "\n",
        "En esta parte del ejercicio, aplicarás PCA a imágenes de rostros para ver cómo puede se puede usar PCA en la práctica para reducción de la dimensionalidad. El archivo `faces.mat` contiene un conjunto de datos `X` con imagénes de 32 x 32 pixeles en tonos de grises. Cada fila corresponde a una imagen (un vector fila de longitud 1024). En el siguiente paso cargaremos `X` y visualizaremos las primeras 100 imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH0m8xPp8Jha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1Bxtbq_L3RCGWDT0cQCQe_7TZdUYY5Am_' -O faces.mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NP4aJwZH4ndM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "ex7faces = loadmat('faces.mat')\n",
        "X = np.array(ex7faces['X'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFDI8hWH4ndO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import figure, imshow, axis\n",
        "\n",
        "def mostrarImagenes(X, num_cols, shape, title='asdf'):\n",
        "    num_imagenes = X.shape[0]\n",
        "    num_filas = np.ceil(num_imagenes / num_cols)\n",
        "    fig = figure()\n",
        "    fig.set_size_inches(10, 10)\n",
        "\n",
        "    for i in range(num_imagenes):\n",
        "        a=fig.add_subplot(num_filas, num_cols, i +1)\n",
        "        image = X[i,:].reshape(shape).T\n",
        "        imshow(image,cmap='Greys_r')\n",
        "        axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYrScOJx4ndR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Imágenes originales')\n",
        "mostrarImagenes(X[:100], 10, [32, 32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpMHtx94ndU",
        "colab_type": "text"
      },
      "source": [
        "### 2.5.1. PCA en rostros \n",
        "\n",
        "Antes de ejecutar PCA en el conjunto de datos, normaliza el conjunto de datos usando la función `scale`. Luego ajusta un modelo PCA a los datos. Observa que cada fila de la matriz de cargas es un vector de longitud $n$ (para el dataset, $n = 1024$). Resulta que podemos visualizar estos componentes principales transformando cada uno de ellos en una matriz 32 x 32 que corresponde a los pixeles del conjunto de datos original. Una vez que hayas ajustado el modelo PCA, mostraremos los primeros 36 componentes principales que describen la mayor varianza. Si deseas, modifica el código para mostrar más componentes principales y ver cómo capturan cada vez más detalles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNdAEXAy4ndV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale \n",
        "\n",
        "X_norm = scale(X)\n",
        "pca_faces = PCA().fit(X_norm)\n",
        "pca_faces_loadings = pca_faces.components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXNxlqcj4ndX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Carga de los primeros 36 componentes principales')\n",
        "mostrarImagenes(pca_faces_loadings[:36,:], num_cols=6, shape=[32, 32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vFhNWiQ4ndZ",
        "colab_type": "text"
      },
      "source": [
        "### Reducción de dimensionalidad\n",
        "\n",
        "Ahora que has calculado los principales componentes del conjunto de datos de rostros, puedes usarlos para reducir su dimensionalidad. Esto te permite entrenar tu algoritmo de aprendizaje con un tamaño de entradas menor (p.ej., 100 dimensiones) en lugar de las 1024 dimensiones originales. Esto puede ayudar a acelerar tu algoritmo de aprendizaje.\n",
        "\n",
        "En la siguiente parte proyectarás el conjunto de datos sobre solamente los primeros 100 componentes principales. Específicamente, cada imagen quedará descrita por un vector $z^{(i)} \\in \\mathbb{R}^{100}$.\n",
        "\n",
        "Para entender lo que se pierde en la reducción de dimensionalidad, recuperarás las imágenes a partir de los datos proyectados.\n",
        "\n",
        "Una vez que hayas calculado las proyecciones y la recuperación de los datos, visualizaremos las imágenes reconstruidas. En la reconstrucción podrás observar que se conserva la estructura general y apariencia de los rostros, y se pierde los detalles más finos. Esta es una notable reducción (más de 10x) en el tamaño del conjunto de datos, que puede ayudar a acelerar significativamente tu algoritmo de aprendizaje. Por ejemplo, si estuvieses entrenando una red neuronal para realizar reconocimiento de personas (dada la imagen de un rostro, identificar a la persona), puedes usar la entrada reducida a 100 dimensiones en lugar de los pixeles originales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAlolts34ndZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Proyecta los datos sobre los primeros 100 componentes principales\n",
        "Z = projectData(pca_faces, X_norm, 100)\n",
        "\n",
        "# Recupera los datos a partir de la proyección\n",
        "X_rec = recoverData(pca_faces, Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgWFzqRM4ndc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Imágenes reconstruidas')\n",
        "mostrarImagenes(X_rec[:100], 10, [32, 32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AX1eyfl4ndf",
        "colab_type": "text"
      },
      "source": [
        "¿Cuál es la proporción acumulada de varianza explicada por los primeros 100 componentes principales del conjunto de datos de rostros? (Redondear a 4 decimales)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPfJ0szK4ndh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('PVE acumulada de los primeros 100 componentes principales: %.4f' % np.cumsum(pca_faces.explained_variance_ratio_)[99] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rReSKLO04ndm",
        "colab_type": "text"
      },
      "source": [
        "¿Puedes graficar la proporción acumulada de varianza explicada por PCA para este conjunto de datos?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_MskhyR4ndn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(np.linspace(1,1024,1024), np.cumsum(pca_faces.explained_variance_ratio_))\n",
        "plt.ylabel('Proporción Acumulada de Varianza Explicada')\n",
        "plt.xlabel('Componente Principal')\n",
        "plt.xlim(0.75,4.25)\n",
        "plt.ylim(0,1.05)\n",
        "plt.xticks(np.linspace(100,1100,11))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}